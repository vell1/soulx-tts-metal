import os
import types
import atexit
from time import perf_counter
from functools import partial
from dataclasses import fields, asdict

import torch
import torch.multiprocessing as mp
from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteriaList
from transformers import EosTokenCriteria, RepetitionPenaltyLogitsProcessor
try:    
    from vllm import LLM
    from vllm import SamplingParams as VllmSamplingParams
    from vllm.inputs import TokensPrompt as TokensPrompt
    SUPPORT_VLLM = True
except ImportError:
    SUPPORT_VLLM = False

from soulxpodcast.config import Config, SamplingParams
from soulxpodcast.models.modules.sampler import _ras_sample_hf_engine

class HFLLMEngine:

    def __init__(self, model, **kwargs):
        config_fields = {field.name for field in fields(Config)}
        config_kwargs = {k: v for k, v in kwargs.items() if k in config_fields}
        config = Config(model, **config_kwargs)
        
        self.tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)
        config.eos = config.hf_config.eos_token_id # speech eos token;
        
        # ðŸ”¥ ä¿®å¤: æ”¯æŒ MPS (Apple Silicon) + æ•°å€¼ç¨³å®šæ€§
        if torch.cuda.is_available():
            self.device = "cuda:0"
            torch_dtype = torch.bfloat16
            print("[INFO] LLM ä½¿ç”¨ CUDA è®¾å¤‡ (bfloat16)")
        elif torch.backends.mps.is_available():
            self.device = "mps"
            # MPS ä¸æ”¯æŒ bfloat16ï¼Œä½¿ç”¨ float16 é¿å… NaN/Inf é”™è¯¯
            torch_dtype = torch.float16
            print("[INFO] LLM ä½¿ç”¨ MPS è®¾å¤‡ (Apple Silicon, float16)")
        else:
            self.device = "cpu"
            torch_dtype = torch.float32
            print("[WARNING] LLM ä½¿ç”¨ CPUï¼ˆæ€§èƒ½è¾ƒå·®ï¼Œfloat32ï¼‰")
        
        self.model = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch_dtype, device_map=self.device)
        self.config = config
        self.pad_token_id = self.tokenizer.pad_token_id

    def generate(
        self,
        prompt: list[str],
        sampling_param: SamplingParams,
        past_key_values=None,
    ) -> dict:
        
        stopping_criteria = StoppingCriteriaList([EosTokenCriteria(eos_token_id=self.config.hf_config.eos_token_id)])
        if sampling_param.use_ras:
            sample_hf_engine_handler = partial(_ras_sample_hf_engine, 
                    use_ras=sampling_param.use_ras, 
                    win_size=sampling_param.win_size, tau_r=sampling_param.tau_r)
        else:
            sample_hf_engine_handler = None
        rep_pen_processor = RepetitionPenaltyLogitsProcessor(
            penalty=sampling_param.repetition_penalty,
            prompt_ignore_length=len(prompt)
        ) # exclude the input prompt, consistent with vLLM implementation;
        with torch.no_grad(): 
            input_len = len(prompt)
            generated_ids = self.model.generate(
                input_ids = torch.tensor([prompt], dtype=torch.int64).to(self.device),
                do_sample=True,
                top_k=sampling_param.top_k,
                top_p=sampling_param.top_p,
                min_new_tokens=sampling_param.min_tokens,
                max_new_tokens=sampling_param.max_tokens,
                temperature=sampling_param.temperature,
                stopping_criteria=stopping_criteria,
                past_key_values=past_key_values,
                custom_generate=sample_hf_engine_handler,
                use_cache=True,
                logits_processor=[rep_pen_processor]
            )
            generated_ids = generated_ids[:, input_len:].cpu().numpy().tolist()[0]
        output = {
            "text": self.tokenizer.decode(generated_ids),
            "token_ids": generated_ids,
        }
        return output

class VLLMEngine:

    def __init__(self, model, **kwargs):
        
        config_fields = {field.name for field in fields(Config)}
        config_kwargs = {k: v for k, v in kwargs.items() if k in config_fields}
        config = Config(model, **config_kwargs)
        
        self.tokenizer = AutoTokenizer.from_pretrained(config.model, use_fast=True)
        config.eos = config.hf_config.eos_token_id # speech eos token;
        
        # ðŸ”¥ ä¿®å¤: ä¸Ž HFLLMEngine ä¿æŒä¸€è‡´çš„è®¾å¤‡æ£€æµ‹
        if torch.cuda.is_available():
            self.device = "cuda:0"
            vllm_dtype = "bfloat16"
            print("[INFO] VLLM ä½¿ç”¨ CUDA è®¾å¤‡ (bfloat16)")
        elif torch.backends.mps.is_available():
            # VLLM ä¸æ”¯æŒ MPSï¼Œå›žé€€åˆ° CPU
            self.device = "cpu"
            vllm_dtype = "float32"
            print("[WARNING] VLLM ä¸æ”¯æŒ MPSï¼Œä½¿ç”¨ CPU (æ€§èƒ½è¾ƒå·®)")
        else:
            self.device = "cpu"
            vllm_dtype = "float32"
            print("[WARNING] VLLM ä½¿ç”¨ CPUï¼ˆæ€§èƒ½è¾ƒå·®ï¼‰")
        
        os.environ["VLLM_USE_V1"] = "0"
        if SUPPORT_VLLM:
            self.model = LLM(model=model, enforce_eager=True, dtype=vllm_dtype, max_model_len=8192)
        else:
            raise ImportError("Not Support VLLM now!!!")
        self.config = config
        self.pad_token_id = self.tokenizer.pad_token_id

    def generate(
        self,
        prompt: list[str],
        sampling_param: SamplingParams,
        past_key_values=None,
    ) -> dict:
        sampling_param.stop_token_ids = [self.config.hf_config.eos_token_id]
        with torch.no_grad():
            generated_ids = self.model.generate(
                TokensPrompt(prompt_token_ids=prompt), 
                VllmSamplingParams(**asdict(sampling_param)),
                use_tqdm=False,
            )[0].outputs[0].token_ids
        output = {
            "text": self.tokenizer.decode(generated_ids),
            "token_ids": list(generated_ids),
        }
        return output